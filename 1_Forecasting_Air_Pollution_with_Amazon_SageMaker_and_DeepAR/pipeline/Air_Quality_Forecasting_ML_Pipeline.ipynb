{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline for \"Forecasting Air Quality with Amazon SageMaker DeepAR\n",
    "\n",
    "In this example, we are going to build a ML Pipeline to automate air quality forecasting application with [AWS Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline\n",
    "\n",
    "### Outcome\n",
    "* Create the flow for ML process for air quality forcasting build/train/deploy\n",
    "* Create simple retrain flow\n",
    "\n",
    "### Design\n",
    "* Use Step Functions Data Science SDK to orchestrate the ML flow\n",
    "* Use SageMaker Processing to do data preprocessing, especially,\n",
    " * A common Docker image will be build for data retrieving (interact with Amazon Athena) and data/feature engineering\n",
    "* Use SageMaker Processing to do Model Evaluation\n",
    "* A scheduled job mechanism will be used to do model retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker==1.71.0\"\n",
    "!{sys.executable} -m pip install -qU \"stepfunctions==1.1.0\"\n",
    "!{sys.executable} -m pip show sagemaker stepfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time\n",
    "import boto3\n",
    "import stepfunctions\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.steps import (\n",
    "    Chain,\n",
    "    ChoiceRule,\n",
    "    ModelStep,\n",
    "    ProcessingStep,\n",
    "    TrainingStep,\n",
    "    TransformStep\n",
    ")\n",
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "role = get_execution_role()\n",
    "\n",
    "session = boto3.Session()\n",
    "account_id = session.client('sts').get_caller_identity().get('Account')\n",
    "bucket_name = f'{account_id}-openaq-lab'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Docker Image for SageMaker Processing\n",
    "\n",
    "Define your own processing container and install related dependencies.\n",
    "\n",
    "Below, you talk through how to create a processing container, and how to use a `ScriptProcessor` to run your own code within a container. Create a container support data preprocessing, feature engineering and model evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subfolder for docker \n",
    "!mkdir -p docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the Dockerfile to create processing container. Install PyAthena, pandas and GeoPandas into it. You can install your own dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "\n",
    "FROM python:3.7-slim-buster\n",
    "    \n",
    "COPY ./sql /opt/ml/processing/sql\n",
    "    \n",
    "RUN pip install pandas numpy geopandas scikit-learn fsspec s3fs boto3\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code buils the container using the docker command, creates an Amazon Elastic Container Registry (Amazon ECR) repository, and pushes the image to Amazon ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_repository = 'aq-forecasting-processing-container'\n",
    "tag = ':latest'\n",
    "\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "processing_repository_uri = f'{account_id}.dkr.ecr.{region}.{uri_suffix}/{ecr_repository + tag}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @todo consider using CFN template to create ECR repo and only manage the docker image build and push.\n",
    "!docker build -t $ecr_repository docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell writes a file `preprocessing.py`, which contains the pre-processing script. You can update the script, and rerun the cell to overwrite `preprocessing.py`. You run this as a processing job in the next cell. In this script, the actions will be done:\n",
    "\n",
    "* Create Athena table with external source - OpenAQ\n",
    "* Query OpenAQ data \n",
    "* Feature engineering on the dataset\n",
    "* Split and store the data on S3 buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the pre processing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING_SCRIPT_LOCATION = \"preprocessing.py\"\n",
    "input_code = sagemaker_session.upload_data(\n",
    "    PREPROCESSING_SCRIPT_LOCATION,\n",
    "    bucket = bucket_name,\n",
    "    key_prefix = \"preprocessing/code\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 locations of preprocessing output with training, test & all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = f\"s3://{bucket_name}/preprocessing/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ScriptProcessor` class lets you run a command inside the container, which you can use to run your own script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "preprocessing_processor = ScriptProcessor(\n",
    "    command = ['python3'],\n",
    "    image_uri = processing_repository_uri,\n",
    "    role = role,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    max_runtime_in_seconds = 1200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ProcessingStep\n",
    "We will now create the [ProcessingStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/sagemaker.html#stepfunctions.steps.sagemaker.ProcessingStep) that will launch a SageMaker Processing Job.\n",
    "\n",
    "This step will use ScriptProcessor as defined in previous steps along with the inputs and outputs objects that are defined in the below steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ProcessingInput(\n",
    "        source = input_code,\n",
    "        destination = \"/opt/ml/processing/input/code\",\n",
    "        input_name = \"code\"\n",
    "    )\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    ProcessingOutput(\n",
    "        source = \"/opt/ml/processing/output/all\",\n",
    "        destination = f\"{output_data}/all\",\n",
    "        output_name = \"all_data\"\n",
    "    ),\n",
    "    ProcessingOutput(\n",
    "        source = \"/opt/ml/processing/output/train\",\n",
    "        destination = f\"{output_data}/train\",\n",
    "        output_name = \"train_data\"\n",
    "    ),\n",
    "    ProcessingOutput(\n",
    "        source = \"/opt/ml/processing/output/test\",\n",
    "        destination = f\"{output_data}/test\",\n",
    "        output_name = \"test_data\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_input = ExecutionInput(\n",
    "    schema = {\n",
    "        \"PreprocessingJobName\": str,\n",
    "        \"ToDoHPO\": bool,\n",
    "        \"ToDoTraining\": bool,\n",
    "        \"TrainingJobName\": str,\n",
    "        \"TuningJobName\": str,\n",
    "        \"ModelName\": str,\n",
    "        \"EndpointName\": str,\n",
    "        \"EvaluationProcessingJobName\": str\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ProcessingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_step = ProcessingStep(\n",
    "    \"AirQualityForecasting Pre-processing Step\",\n",
    "    processor = preprocessing_processor,\n",
    "    job_name = execution_input[\"PreprocessingJobName\"],\n",
    "    inputs = inputs,\n",
    "    outputs = outputs,\n",
    "    container_arguments = [\"--split-days\", \"30\"],\n",
    "    container_entrypoint = [\"python3\", \"/opt/ml/processing/input/code/preprocessing.py\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter Tuning\n",
    "\n",
    "Setup tuning step and use choice state to decide whether we should do HPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We create a DeepAR instance, which we will use to run a training job. This will be used to create a TrainingStep for the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter\n",
    "from sagemaker.model import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f's3://{bucket_name}/tuning/output'\n",
    "\n",
    "ml_instance_type = 'ml.c5.9xlarge'\n",
    "\n",
    "tunning_estimator = sagemaker.estimator.Estimator(\n",
    "        sagemaker_session = sagemaker_session,\n",
    "        image_name = image_name,\n",
    "        role = role,\n",
    "        train_instance_count = 1,\n",
    "        train_instance_type = ml_instance_type,\n",
    "        base_job_name = 'deepar-openaq-demo',\n",
    "        output_path = output_path\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set static hyperparameters\n",
    "The static parameters are the ones we know to be the best based on previously run HPO jobs, as well as the non-tunable parameters like prediction length and time frequency that are set according to requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo = dict(\n",
    "    time_freq= '1H'\n",
    "    ,early_stopping_patience= 40\n",
    "    ,prediction_length= 48\n",
    "    ,num_eval_samples= 10\n",
    "\n",
    "    # default quantiles [0.1, 0.2, 0.3, ..., 0.9] is used\n",
    "    #,test_quantiles= quantiles\n",
    "    \n",
    "    # not setting these since HPO will use range of values\n",
    "    #,epochs= 400\n",
    "    #,context_length= 3\n",
    "    #,num_cells= 157\n",
    "    #,num_layers= 4\n",
    "    #,dropout_rate= 0.04\n",
    "    #,embedding_dimension= 12\n",
    "    #,mini_batch_size= 633\n",
    "    #,learning_rate= 0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyper-parameter ranges\n",
    "The hyperparameter ranges define the parameters we want the runer to search across.\n",
    "\n",
    "> Explore: Look in the [user guide](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html) for DeepAR and add the recommended ranges for `embedding_dimension` to the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_ranges = dict(\n",
    "    epochs= IntegerParameter(1, 1000)\n",
    "    ,context_length= IntegerParameter(7, 48)\n",
    "    ,num_cells= IntegerParameter(30,200)\n",
    "    ,num_layers= IntegerParameter(1,8)\n",
    "    ,dropout_rate= ContinuousParameter(0.0, 0.2)\n",
    "    ,embedding_dimension= IntegerParameter(1, 50)\n",
    "    ,mini_batch_size= IntegerParameter(32, 1028)\n",
    "    ,learning_rate= ContinuousParameter(.00001, .1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create HPO tunning job step\n",
    "Once we have the HPO tuner defined, we can define the tuning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_estimator.set_hyperparameters(**hpo)\n",
    "\n",
    "hpo_tuner = tuning_estimator(\n",
    "    estimator= estimator, \n",
    "    objective_metric_name = 'train:final_loss',\n",
    "    objective_type = 'Minimize',\n",
    "    hyperparameter_ranges = hpo_ranges,\n",
    "    max_jobs = 2,\n",
    "    max_parallel_jobs = 1\n",
    ")\n",
    "\n",
    "hpo_data = dict(\n",
    "    train = f\"{output_data}/train\",\n",
    "    test = f\"{output_data}/test\"\n",
    ")\n",
    "# as long as HPO is selected, wait for completion.\n",
    "tuning_step = steps.TuningStep(\n",
    "    \"Tuning Step\",\n",
    "    tuner = hpo_tuner,\n",
    "    job_name = execution_input[\"TuningJobName\"],\n",
    "    data = hpo_data,\n",
    "    wait_for_completion = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the training job step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f's3://{bucket_name}/training/output'\n",
    "training_estimator = sagemaker.estimator.Estimator(\n",
    "        sagemaker_session = sagemaker_session,\n",
    "        image_name = image_name,\n",
    "        role = role,\n",
    "        train_instance_count = 1,\n",
    "        train_instance_type = ml_instance_type,\n",
    "        base_job_name = 'deepar-openaq-demo',\n",
    "        output_path = output_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best hyper parameters for tuning\n",
    "hpo = dict(\n",
    "    time_freq= '1H'\n",
    "    ,early_stopping_patience= 40\n",
    "    ,prediction_length= 48\n",
    "    ,num_eval_samples= 10\n",
    "    #,test_quantiles= quantiles\n",
    "    ,epochs= 400\n",
    "    ,context_length= 3\n",
    "    ,num_cells= 157\n",
    "    ,num_layers= 4\n",
    "    ,dropout_rate= 0.04\n",
    "    ,embedding_dimension= 12\n",
    "    ,mini_batch_size= 633\n",
    "    ,learning_rate= 0.0005\n",
    ")\n",
    "estimator.set_hyperparameters(**hpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all the features for training.\n",
    "data = dict(train = f\"{output_data}/all/all_features.json\")\n",
    "training_step = steps.TrainingStep(\n",
    "    \"Training Step\",\n",
    "    estimator = estimator,\n",
    "    data = data,\n",
    "    job_name = execution_input[\"TrainingJobName\"],\n",
    "    wait_for_completion = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Model Step\n",
    "\n",
    "In the following cell, we define a model step that will create a model in Amazon SageMaker using the artifacts created during the TrainingStep. See  [ModelStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.ModelStep) in the AWS Step Functions Data Science SDK documentation to learn more.\n",
    "\n",
    "The model creation step typically follows the training step. The Step Functions SDK provides the [get_expected_model](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TrainingStep.get_expected_model) method in the TrainingStep class to provide a reference for the trained model artifacts. Please note that this method is only useful when the ModelStep directly follows the TrainingStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_step = steps.ModelStep(\n",
    "    \"Save Model\",\n",
    "    model = training_step.get_expected_model(),\n",
    "    model_name = execution_input[\"ModelName\"],\n",
    "    result_path = \"$.ModelStepResults\"\n",
    ")\n",
    "\n",
    "existing_model_name = f\"aqf-model-{uuid.uuid1().hex}\"\n",
    "existing_model = Model(\n",
    "    model_data = f\"s3//{bucket_name}/sagemaker/model/model.tar.gz\",\n",
    "    image = image_name,\n",
    "    role = role,\n",
    "    name = existing_model_name\n",
    ")\n",
    "existing_model_step = steps.ModelStep(\n",
    "    \"Existing Model\",\n",
    "    model = existing_model,\n",
    "    model_name = execution_input[\"ModelName\"],\n",
    "    result_path = \"$.ModelStepResults\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Endpoint Configuration Step\n",
    "In the following cell we create an endpoint configuration step. See [EndpointConfigStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.EndpointConfigStep) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_step = steps.EndpointConfigStep(\n",
    "    \"Create Model Endpoint Config\",\n",
    "    endpoint_config_name = execution_input[\"ModelName\"],\n",
    "    model_name = execution_input[\"ModelName\"],\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.c5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the Model Endpoint Step\n",
    "In the following cell, we create the Endpoint step to deploy the new model as a managed API endpoint, updating an existing SageMaker endpoint if our choice state is sucessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_step = steps.EndpointStep(\n",
    "    \"Update Model Endpoint\",\n",
    "    endpoint_name = execution_input[\"EndpointName\"],\n",
    "    endpoint_config_name = execution_input[\"ModelName\"],\n",
    "    update = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "Run model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup workflow process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `Fail` state to mark the workflow failed in case any of the steps fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_state_sagemaker_processing_failure = stepfunctions.steps.states.Fail(\n",
    "    \"ML Workflow failed\", cause = \"SageMakerProcessingJobFailed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = Chain([training_step, model_step, endpoint_config_step, endpoint_step])\n",
    "deploy_existing_model_path = Chain([existing_model_step, endpoint_config_step, endpoint_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice state\n",
    "\n",
    "Now, we need to setup choice state for choose HPO / Training or not. See *Choice Rules* in the [AWS Step Functions Data Science SDK documentation](https://aws-step-functions-data-science-sdk.readthedocs.io) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_choice = Choice(\n",
    "    \"To do HPO?\"\n",
    ")\n",
    "training_choice = Choice(\n",
    "    \"To do Model Training?\"\n",
    ")\n",
    "hpo_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = execution_input['ToDoHPO'], value = True),\n",
    "    next_step = hpo_step\n",
    ")\n",
    "hpo_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = execution_input['ToDoHPO'], value = False),\n",
    "    next_step = training_choice\n",
    ")\n",
    "training_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = execution_input['ToDoTraining'], value = True),\n",
    "    next_step = training_path\n",
    ")\n",
    "training_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = execution_input['ToDoTraining'], value = False),\n",
    "    next_step = deploy_existing_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the Error handling in the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_state_processing = stepfunctions.steps.states.Catch(\n",
    "    error_equals = [\"States.TaskFailed\"],\n",
    "    next_step = failed_state_sagemaker_processing_failure   \n",
    ")\n",
    "processing_step.add_catch(catch_state_processing)\n",
    "training_step.add_catch(catch_state_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_execution_role = \"arn:aws:iam::593380422482:role/StepFunctionsWorkflowExecutionRole\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create StepFunctions Workflow execution Input schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_job_name = f\"aqf-preprocessing-{uuid.uuid1().hex}\"\n",
    "tuning_job_name = f\"aqf-tuning-{uuid.uuid1().hex}\"\n",
    "training_job_name = f\"aqf-training-{uuid.uuid1().hex}\"\n",
    "model_job_name = f\"aqf-model-{uuid.uuid1().hex}\"\n",
    "endpoint_job_name = f\"aqf-endpoint-{uuid.uuid1().hex}\"\n",
    "evaluation_job_name = f\"aqf-evaluation-{uuid.uuid1().hex}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preprocessing_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and execute the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_graph = Chain([processing_step, hpo_choice, training_choice])\n",
    "workflow = Workflow(\n",
    "    name = \"AirQualityForecastingWorkflow\",\n",
    "    definition = workflow_graph,\n",
    "    role = workflow_execution_role\n",
    ")\n",
    "workflow.create()\n",
    "# update() to ensure existing workflow can get updated as create() just return ARN for the existing one.\n",
    "workflow.update(definition = workflow_graph) \n",
    "\n",
    "# execute workflow\n",
    "execution = workflow.execute(\n",
    "    inputs = {\n",
    "        \"PreprocessingJobName\": preprocessing_job_name,\n",
    "        \"ToDoHPO\": False,\n",
    "        \"ToDoTraining\": False,\n",
    "        \"TrainingJobName\": training_job_name,\n",
    "        \"TuningJobName\": tuning_job_name,\n",
    "        \"ModelName\": model_job_name,\n",
    "        \"EndpointName\": endpoint_job_name,\n",
    "        \"EvaluationProcessingJobName\": evaluation_job_name\n",
    "    }\n",
    ")\n",
    "execution_output = execution.get_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
